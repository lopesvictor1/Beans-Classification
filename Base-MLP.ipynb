{"cells":[{"cell_type":"markdown","metadata":{"id":"VMpZC5eq6HNR"},"source":["Experimento 2 - KNN Imputer, 3sigma outlier detection, min max normalization, MLP classifier"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":26715,"status":"ok","timestamp":1704299618201,"user":{"displayName":"Victor Hugo Schneider Lopes","userId":"10992764016890176856"},"user_tz":180},"id":"cP53TLVS5xq-"},"outputs":[],"source":["import pandas as pd\n","import seaborn as sns\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn.objects as so\n","from ucimlrepo import fetch_ucirepo"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1061,"status":"ok","timestamp":1704299619253,"user":{"displayName":"Victor Hugo Schneider Lopes","userId":"10992764016890176856"},"user_tz":180},"id":"0owuQnJm6Vl5"},"outputs":[],"source":["beans = fetch_ucirepo(id=602)\n","df = beans.data.features\n","targets = beans.data.targets"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1704299619254,"user":{"displayName":"Victor Hugo Schneider Lopes","userId":"10992764016890176856"},"user_tz":180},"id":"_jMLf_dB6aIn"},"outputs":[],"source":["cols = ['Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength', 'AspectRatio', 'Eccentricity', 'ConvexArea', 'EquivDiameter', 'Extent', 'Solidity', 'Roundness', 'Compactness', 'ShapeFactor1', 'ShapeFactor2', 'ShapeFactor3', 'ShapeFactor4']"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1704299643303,"user":{"displayName":"Victor Hugo Schneider Lopes","userId":"10992764016890176856"},"user_tz":180},"id":"dsT5e4MCtxV0","outputId":"ad21aa9a-ce69-49f2-a163-52b65db38016"},"outputs":[{"name":"stdout","output_type":"stream","text":["          Area Perimeter MajorAxisLength MinorAxisLength AspectRatio  \\\n","         count     count           count           count       count   \n","Class                                                                  \n","BARBUNYA  1322      1322            1322            1322        1322   \n","BOMBAY     522       522             522             522         522   \n","CALI      1630      1630            1630            1630        1630   \n","DERMASON  3546      3546            3546            3546        3546   \n","HOROZ     1928      1928            1928            1928        1928   \n","SEKER     2027      2027            2027            2027        2027   \n","SIRA      2636      2636            2636            2636        2636   \n","\n","         Eccentricity ConvexArea EquivDiameter Extent Solidity Roundness  \\\n","                count      count         count  count    count     count   \n","Class                                                                      \n","BARBUNYA         1322       1322          1322   1322     1322      1322   \n","BOMBAY            522        522           522    522      522       522   \n","CALI             1630       1630          1630   1630     1630      1630   \n","DERMASON         3546       3546          3546   3546     3546      3546   \n","HOROZ            1928       1928          1928   1928     1928      1928   \n","SEKER            2027       2027          2027   2027     2027      2027   \n","SIRA             2636       2636          2636   2636     2636      2636   \n","\n","         Compactness ShapeFactor1 ShapeFactor2 ShapeFactor3 ShapeFactor4  \n","               count        count        count        count        count  \n","Class                                                                     \n","BARBUNYA        1322         1322         1322         1322         1322  \n","BOMBAY           522          522          522          522          522  \n","CALI            1630         1630         1630         1630         1630  \n","DERMASON        3546         3546         3546         3546         3546  \n","HOROZ           1928         1928         1928         1928         1928  \n","SEKER           2027         2027         2027         2027         2027  \n","SIRA            2636         2636         2636         2636         2636  \n"]}],"source":["#adding the labels\n","df['Class'] = targets\n","df_pre_missing_values = df.copy()\n","print(df.groupby('Class').agg(['count']))\n","df['Class'] = df['Class'].transform(lambda x: 0 if x == 'BARBUNYA' else (1 if x == 'BOMBAY' else (2 if x == 'CALI' else (3 if x == 'DERMASON' else (4 if x == 'HOROZ' else (5 if x == 'SEKER' else 6))))))"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":444},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1704299643773,"user":{"displayName":"Victor Hugo Schneider Lopes","userId":"10992764016890176856"},"user_tz":180},"id":"nSe3jkEjynF6","outputId":"04b62876-29dc-4492-f654-eab5eaf38733"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Area</th>\n","      <th>Perimeter</th>\n","      <th>MajorAxisLength</th>\n","      <th>MinorAxisLength</th>\n","      <th>AspectRatio</th>\n","      <th>Eccentricity</th>\n","      <th>ConvexArea</th>\n","      <th>EquivDiameter</th>\n","      <th>Extent</th>\n","      <th>Solidity</th>\n","      <th>Roundness</th>\n","      <th>Compactness</th>\n","      <th>ShapeFactor1</th>\n","      <th>ShapeFactor2</th>\n","      <th>ShapeFactor3</th>\n","      <th>ShapeFactor4</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.034053</td>\n","      <td>0.058574</td>\n","      <td>0.044262</td>\n","      <td>0.152142</td>\n","      <td>0.122612</td>\n","      <td>0.477797</td>\n","      <td>0.033107</td>\n","      <td>0.070804</td>\n","      <td>0.671024</td>\n","      <td>0.922824</td>\n","      <td>0.934823</td>\n","      <td>0.786733</td>\n","      <td>0.593432</td>\n","      <td>0.833049</td>\n","      <td>0.750996</td>\n","      <td>0.980620</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.035500</td>\n","      <td>0.077557</td>\n","      <td>0.030479</td>\n","      <td>0.178337</td>\n","      <td>0.051577</td>\n","      <td>0.278472</td>\n","      <td>0.034991</td>\n","      <td>0.073577</td>\n","      <td>0.735504</td>\n","      <td>0.871514</td>\n","      <td>0.793138</td>\n","      <td>0.903549</td>\n","      <td>0.547447</td>\n","      <td>0.967316</td>\n","      <td>0.884987</td>\n","      <td>0.974979</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.038259</td>\n","      <td>0.068035</td>\n","      <td>0.052633</td>\n","      <td>0.158190</td>\n","      <td>0.131521</td>\n","      <td>0.496448</td>\n","      <td>0.037126</td>\n","      <td>0.078816</td>\n","      <td>0.716671</td>\n","      <td>0.932141</td>\n","      <td>0.914511</td>\n","      <td>0.773514</td>\n","      <td>0.582016</td>\n","      <td>0.800942</td>\n","      <td>0.736200</td>\n","      <td>0.987196</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.040940</td>\n","      <td>0.082942</td>\n","      <td>0.048548</td>\n","      <td>0.177691</td>\n","      <td>0.091623</td>\n","      <td>0.403864</td>\n","      <td>0.041389</td>\n","      <td>0.083854</td>\n","      <td>0.731365</td>\n","      <td>0.761614</td>\n","      <td>0.826871</td>\n","      <td>0.829912</td>\n","      <td>0.552408</td>\n","      <td>0.854744</td>\n","      <td>0.799846</td>\n","      <td>0.893675</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.041504</td>\n","      <td>0.065313</td>\n","      <td>0.032862</td>\n","      <td>0.200679</td>\n","      <td>0.025565</td>\n","      <td>0.165680</td>\n","      <td>0.040123</td>\n","      <td>0.084906</td>\n","      <td>0.700538</td>\n","      <td>0.949832</td>\n","      <td>0.988408</td>\n","      <td>0.951583</td>\n","      <td>0.510741</td>\n","      <td>1.000000</td>\n","      <td>0.941770</td>\n","      <td>0.989116</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>13606</th>\n","      <td>0.092559</td>\n","      <td>0.160862</td>\n","      <td>0.189318</td>\n","      <td>0.187843</td>\n","      <td>0.375584</td>\n","      <td>0.788553</td>\n","      <td>0.089967</td>\n","      <td>0.172180</td>\n","      <td>0.512286</td>\n","      <td>0.942381</td>\n","      <td>0.852151</td>\n","      <td>0.465175</td>\n","      <td>0.531785</td>\n","      <td>0.382135</td>\n","      <td>0.412185</td>\n","      <td>0.974113</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>13607</th>\n","      <td>0.092576</td>\n","      <td>0.159358</td>\n","      <td>0.176450</td>\n","      <td>0.201964</td>\n","      <td>0.321303</td>\n","      <td>0.746241</td>\n","      <td>0.089910</td>\n","      <td>0.172207</td>\n","      <td>0.786890</td>\n","      <td>0.947954</td>\n","      <td>0.862952</td>\n","      <td>0.523974</td>\n","      <td>0.509582</td>\n","      <td>0.426233</td>\n","      <td>0.470848</td>\n","      <td>0.970912</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>13608</th>\n","      <td>0.092739</td>\n","      <td>0.160605</td>\n","      <td>0.176384</td>\n","      <td>0.203370</td>\n","      <td>0.318558</td>\n","      <td>0.743877</td>\n","      <td>0.090219</td>\n","      <td>0.172463</td>\n","      <td>0.561689</td>\n","      <td>0.936648</td>\n","      <td>0.855785</td>\n","      <td>0.525351</td>\n","      <td>0.508683</td>\n","      <td>0.427019</td>\n","      <td>0.472240</td>\n","      <td>0.943025</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>13609</th>\n","      <td>0.092773</td>\n","      <td>0.163657</td>\n","      <td>0.179703</td>\n","      <td>0.200669</td>\n","      <td>0.330472</td>\n","      <td>0.753971</td>\n","      <td>0.090623</td>\n","      <td>0.172517</td>\n","      <td>0.482741</td>\n","      <td>0.908991</td>\n","      <td>0.834795</td>\n","      <td>0.510145</td>\n","      <td>0.514216</td>\n","      <td>0.415330</td>\n","      <td>0.456919</td>\n","      <td>0.913342</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>13610</th>\n","      <td>0.092824</td>\n","      <td>0.169448</td>\n","      <td>0.200882</td>\n","      <td>0.176768</td>\n","      <td>0.423337</td>\n","      <td>0.819877</td>\n","      <td>0.090347</td>\n","      <td>0.172598</td>\n","      <td>0.751569</td>\n","      <td>0.933322</td>\n","      <td>0.795826</td>\n","      <td>0.416526</td>\n","      <td>0.550320</td>\n","      <td>0.346892</td>\n","      <td>0.364762</td>\n","      <td>0.970162</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>13611 rows × 17 columns</p>\n","</div>"],"text/plain":["           Area  Perimeter  MajorAxisLength  MinorAxisLength  AspectRatio  \\\n","0      0.034053   0.058574         0.044262         0.152142     0.122612   \n","1      0.035500   0.077557         0.030479         0.178337     0.051577   \n","2      0.038259   0.068035         0.052633         0.158190     0.131521   \n","3      0.040940   0.082942         0.048548         0.177691     0.091623   \n","4      0.041504   0.065313         0.032862         0.200679     0.025565   \n","...         ...        ...              ...              ...          ...   \n","13606  0.092559   0.160862         0.189318         0.187843     0.375584   \n","13607  0.092576   0.159358         0.176450         0.201964     0.321303   \n","13608  0.092739   0.160605         0.176384         0.203370     0.318558   \n","13609  0.092773   0.163657         0.179703         0.200669     0.330472   \n","13610  0.092824   0.169448         0.200882         0.176768     0.423337   \n","\n","       Eccentricity  ConvexArea  EquivDiameter    Extent  Solidity  Roundness  \\\n","0          0.477797    0.033107       0.070804  0.671024  0.922824   0.934823   \n","1          0.278472    0.034991       0.073577  0.735504  0.871514   0.793138   \n","2          0.496448    0.037126       0.078816  0.716671  0.932141   0.914511   \n","3          0.403864    0.041389       0.083854  0.731365  0.761614   0.826871   \n","4          0.165680    0.040123       0.084906  0.700538  0.949832   0.988408   \n","...             ...         ...            ...       ...       ...        ...   \n","13606      0.788553    0.089967       0.172180  0.512286  0.942381   0.852151   \n","13607      0.746241    0.089910       0.172207  0.786890  0.947954   0.862952   \n","13608      0.743877    0.090219       0.172463  0.561689  0.936648   0.855785   \n","13609      0.753971    0.090623       0.172517  0.482741  0.908991   0.834795   \n","13610      0.819877    0.090347       0.172598  0.751569  0.933322   0.795826   \n","\n","       Compactness  ShapeFactor1  ShapeFactor2  ShapeFactor3  ShapeFactor4  \\\n","0         0.786733      0.593432      0.833049      0.750996      0.980620   \n","1         0.903549      0.547447      0.967316      0.884987      0.974979   \n","2         0.773514      0.582016      0.800942      0.736200      0.987196   \n","3         0.829912      0.552408      0.854744      0.799846      0.893675   \n","4         0.951583      0.510741      1.000000      0.941770      0.989116   \n","...            ...           ...           ...           ...           ...   \n","13606     0.465175      0.531785      0.382135      0.412185      0.974113   \n","13607     0.523974      0.509582      0.426233      0.470848      0.970912   \n","13608     0.525351      0.508683      0.427019      0.472240      0.943025   \n","13609     0.510145      0.514216      0.415330      0.456919      0.913342   \n","13610     0.416526      0.550320      0.346892      0.364762      0.970162   \n","\n","       Class  \n","0          5  \n","1          5  \n","2          5  \n","3          5  \n","4          5  \n","...      ...  \n","13606      3  \n","13607      3  \n","13608      3  \n","13609      3  \n","13610      3  \n","\n","[13611 rows x 17 columns]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["#Normalization with min max\n","from sklearn.preprocessing import MinMaxScaler\n","\n","scaler = MinMaxScaler()\n","scaler.fit_transform(df[cols])\n","df_scaled = pd.DataFrame(scaler.transform(df[cols]), columns = cols)\n","\n","df_scaled['Class'] = df['Class']\n","df_scaled"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25560,"status":"ok","timestamp":1704300146535,"user":{"displayName":"Victor Hugo Schneider Lopes","userId":"10992764016890176856"},"user_tz":180},"id":"0dBs8lW22D02","outputId":"8d6a3888-7680-4a0b-f55b-0d129742eeda"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration 1, loss = 1.39141671\n","Iteration 2, loss = 1.03304955\n","Iteration 3, loss = 0.96023082\n","Iteration 4, loss = 0.86651706\n","Iteration 5, loss = 0.82105824\n","Iteration 6, loss = 0.79003165\n","Iteration 7, loss = 0.77562245\n","Iteration 8, loss = 0.79078257\n","Iteration 9, loss = 0.76586878\n","Iteration 10, loss = 0.73905027\n","Iteration 11, loss = 0.74094906\n","Iteration 12, loss = 0.73931242\n","Iteration 13, loss = 0.72808170\n","Iteration 14, loss = 0.72651871\n","Iteration 15, loss = 0.72156565\n","Iteration 16, loss = 0.73119984\n","Iteration 17, loss = 0.72374600\n","Iteration 18, loss = 0.71303625\n","Iteration 19, loss = 0.72941542\n","Iteration 20, loss = 0.69478531\n","Iteration 21, loss = 0.70380477\n","Iteration 22, loss = 0.69875763\n","Iteration 23, loss = 0.71682303\n","Iteration 24, loss = 0.70617741\n","Iteration 25, loss = 0.70696931\n","Iteration 26, loss = 0.68399768\n","Iteration 27, loss = 0.73331474\n","Iteration 28, loss = 0.71176157\n","Iteration 29, loss = 0.71603196\n","Iteration 30, loss = 0.70725045\n","Iteration 31, loss = 0.73028709\n","Iteration 32, loss = 0.71257890\n","Iteration 33, loss = 0.70373183\n","Iteration 34, loss = 0.70574995\n","Iteration 35, loss = 0.72474402\n","Iteration 36, loss = 0.67488888\n","Iteration 37, loss = 0.66927671\n","Iteration 38, loss = 0.74537420\n","Iteration 39, loss = 0.77463394\n","Iteration 40, loss = 0.76712949\n","Iteration 41, loss = 0.66121354\n","Iteration 42, loss = 0.64070447\n","Iteration 43, loss = 0.59485640\n","Iteration 44, loss = 0.57162316\n","Iteration 45, loss = 0.57682453\n","Iteration 46, loss = 0.53456443\n","Iteration 47, loss = 0.50799913\n","Iteration 48, loss = 0.52146017\n","Iteration 49, loss = 0.49946544\n","Iteration 50, loss = 0.47945154\n","Iteration 51, loss = 0.47455712\n","Iteration 52, loss = 0.49323936\n","Iteration 53, loss = 0.48510205\n","Iteration 54, loss = 0.47040059\n","Iteration 55, loss = 0.47240985\n","Iteration 56, loss = 0.43854126\n","Iteration 57, loss = 0.43931781\n","Iteration 58, loss = 0.45558531\n","Iteration 59, loss = 0.43244144\n","Iteration 60, loss = 0.44365368\n","Iteration 61, loss = 0.41373030\n","Iteration 62, loss = 0.42645887\n","Iteration 63, loss = 0.41633337\n","Iteration 64, loss = 0.41827483\n","Iteration 65, loss = 0.43423764\n","Iteration 66, loss = 0.50630728\n","Iteration 67, loss = 0.41916026\n","Iteration 68, loss = 0.42001684\n","Iteration 69, loss = 0.39453544\n","Iteration 70, loss = 0.38252848\n","Iteration 71, loss = 0.40117349\n","Iteration 72, loss = 0.49901676\n","Iteration 73, loss = 0.43708827\n","Iteration 74, loss = 0.39298352\n","Iteration 75, loss = 0.39493626\n","Iteration 76, loss = 0.41835601\n","Iteration 77, loss = 0.38394939\n","Iteration 78, loss = 0.48589437\n","Iteration 79, loss = 0.42253349\n","Iteration 80, loss = 0.39581307\n","Iteration 81, loss = 0.44037836\n","Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 1.24716586\n","Iteration 2, loss = 0.94675339\n","Iteration 3, loss = 0.84989987\n","Iteration 4, loss = 0.73636321\n","Iteration 5, loss = 0.58700568\n","Iteration 6, loss = 0.51389724\n","Iteration 7, loss = 0.43624995\n","Iteration 8, loss = 0.41784140\n","Iteration 9, loss = 0.41483122\n","Iteration 10, loss = 0.40517112\n","Iteration 11, loss = 0.38053505\n","Iteration 12, loss = 0.37886901\n","Iteration 13, loss = 0.35187508\n","Iteration 14, loss = 0.34388330\n","Iteration 15, loss = 0.35117174\n","Iteration 16, loss = 0.33582753\n","Iteration 17, loss = 0.33939965\n","Iteration 18, loss = 0.32753269\n","Iteration 19, loss = 0.31892962\n","Iteration 20, loss = 0.32775380\n","Iteration 21, loss = 0.35861476\n","Iteration 22, loss = 0.33194323\n","Iteration 23, loss = 0.33229344\n","Iteration 24, loss = 0.30793935\n","Iteration 25, loss = 0.32042182\n","Iteration 26, loss = 0.31206832\n","Iteration 27, loss = 0.32809051\n","Iteration 28, loss = 0.31334951\n","Iteration 29, loss = 0.28710877\n","Iteration 30, loss = 0.29858312\n","Iteration 31, loss = 0.31454271\n","Iteration 32, loss = 0.31504994\n","Iteration 33, loss = 0.32832471\n","Iteration 34, loss = 0.29956150\n","Iteration 35, loss = 0.31900430\n","Iteration 36, loss = 0.31247984\n","Iteration 37, loss = 0.30237571\n","Iteration 38, loss = 0.31570268\n","Iteration 39, loss = 0.29892240\n","Iteration 40, loss = 0.28803240\n","Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 1.25967992\n","Iteration 2, loss = 1.00570285\n","Iteration 3, loss = 0.76647809\n","Iteration 4, loss = 0.62263955\n","Iteration 5, loss = 0.56651127\n","Iteration 6, loss = 0.54697698\n","Iteration 7, loss = 0.52068263\n","Iteration 8, loss = 0.49730290\n","Iteration 9, loss = 0.48665109\n","Iteration 10, loss = 0.45925174\n","Iteration 11, loss = 0.45375577\n","Iteration 12, loss = 0.46446047\n","Iteration 13, loss = 0.42647949\n","Iteration 14, loss = 0.39574344\n","Iteration 15, loss = 0.40889180\n","Iteration 16, loss = 0.46233506\n","Iteration 17, loss = 0.40890774\n","Iteration 18, loss = 0.38431012\n","Iteration 19, loss = 0.44761040\n","Iteration 20, loss = 0.36974024\n","Iteration 21, loss = 0.38543349\n","Iteration 22, loss = 0.37709360\n","Iteration 23, loss = 0.44791983\n","Iteration 24, loss = 0.34930940\n","Iteration 25, loss = 0.38018835\n","Iteration 26, loss = 0.35116935\n","Iteration 27, loss = 0.40569938\n","Iteration 28, loss = 0.37369389\n","Iteration 29, loss = 0.34037110\n","Iteration 30, loss = 0.34286950\n","Iteration 31, loss = 0.34928496\n","Iteration 32, loss = 0.36869510\n","Iteration 33, loss = 0.36621449\n","Iteration 34, loss = 0.34892876\n","Iteration 35, loss = 0.36418782\n","Iteration 36, loss = 0.33889060\n","Iteration 37, loss = 0.33887609\n","Iteration 38, loss = 0.33691589\n","Iteration 39, loss = 0.33227414\n","Iteration 40, loss = 0.34844908\n","Iteration 41, loss = 0.32406867\n","Iteration 42, loss = 0.35040950\n","Iteration 43, loss = 0.37138069\n","Iteration 44, loss = 0.36051146\n","Iteration 45, loss = 0.36489544\n","Iteration 46, loss = 0.34989309\n","Iteration 47, loss = 0.32950429\n","Iteration 48, loss = 0.38983778\n","Iteration 49, loss = 0.33652661\n","Iteration 50, loss = 0.34127467\n","Iteration 51, loss = 0.35808331\n","Iteration 52, loss = 0.35745259\n","Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 1.25659305\n","Iteration 2, loss = 0.97121518\n","Iteration 3, loss = 0.80729244\n","Iteration 4, loss = 0.65168609\n","Iteration 5, loss = 0.53886141\n","Iteration 6, loss = 0.52918241\n","Iteration 7, loss = 0.46961823\n","Iteration 8, loss = 0.44585827\n","Iteration 9, loss = 0.43066593\n","Iteration 10, loss = 0.40453111\n","Iteration 11, loss = 0.42985343\n","Iteration 12, loss = 0.39215169\n","Iteration 13, loss = 0.37076642\n","Iteration 14, loss = 0.35141554\n","Iteration 15, loss = 0.36423191\n","Iteration 16, loss = 0.33702511\n","Iteration 17, loss = 0.35921350\n","Iteration 18, loss = 0.32158351\n","Iteration 19, loss = 0.33106311\n","Iteration 20, loss = 0.31938666\n","Iteration 21, loss = 0.35017416\n","Iteration 22, loss = 0.36212240\n","Iteration 23, loss = 0.36368693\n","Iteration 24, loss = 0.32789539\n","Iteration 25, loss = 0.33449572\n","Iteration 26, loss = 0.30866010\n","Iteration 27, loss = 0.32902963\n","Iteration 28, loss = 0.32369308\n","Iteration 29, loss = 0.31988275\n","Iteration 30, loss = 0.31391592\n","Iteration 31, loss = 0.32146995\n","Iteration 32, loss = 0.31870334\n","Iteration 33, loss = 0.37673774\n","Iteration 34, loss = 0.33556327\n","Iteration 35, loss = 0.32647848\n","Iteration 36, loss = 0.31896056\n","Iteration 37, loss = 0.32577567\n","Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 1.26003726\n","Iteration 2, loss = 0.96701988\n","Iteration 3, loss = 0.81744925\n","Iteration 4, loss = 0.71247658\n","Iteration 5, loss = 0.57136226\n","Iteration 6, loss = 0.57119106\n","Iteration 7, loss = 0.55091839\n","Iteration 8, loss = 0.47126349\n","Iteration 9, loss = 0.46998203\n","Iteration 10, loss = 0.45795196\n","Iteration 11, loss = 0.47585347\n","Iteration 12, loss = 0.43838391\n","Iteration 13, loss = 0.42559486\n","Iteration 14, loss = 0.41378807\n","Iteration 15, loss = 0.41465891\n","Iteration 16, loss = 0.39572114\n","Iteration 17, loss = 0.39209304\n","Iteration 18, loss = 0.40311176\n","Iteration 19, loss = 0.37562573\n","Iteration 20, loss = 0.38285313\n","Iteration 21, loss = 0.38728935\n","Iteration 22, loss = 0.37241354\n","Iteration 23, loss = 0.37363902\n","Iteration 24, loss = 0.39483377\n","Iteration 25, loss = 0.36121395\n","Iteration 26, loss = 0.37360868\n","Iteration 27, loss = 0.43602411\n","Iteration 28, loss = 0.35215727\n","Iteration 29, loss = 0.36359315\n","Iteration 30, loss = 0.37517830\n","Iteration 31, loss = 0.35520038\n","Iteration 32, loss = 0.36718505\n","Iteration 33, loss = 0.39375837\n","Iteration 34, loss = 0.36067384\n","Iteration 35, loss = 0.37569742\n","Iteration 36, loss = 0.35904160\n","Iteration 37, loss = 0.36195143\n","Iteration 38, loss = 0.34820829\n","Iteration 39, loss = 0.34812632\n","Iteration 40, loss = 0.35121059\n","Iteration 41, loss = 0.35314092\n","Iteration 42, loss = 0.41795850\n","Iteration 43, loss = 0.36382453\n","Iteration 44, loss = 0.36146502\n","Iteration 45, loss = 0.38165738\n","Iteration 46, loss = 0.37607151\n","Iteration 47, loss = 0.35050254\n","Iteration 48, loss = 0.37953819\n","Iteration 49, loss = 0.35890499\n","Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 1.26984993\n","Iteration 2, loss = 0.92934919\n","Iteration 3, loss = 0.74190820\n","Iteration 4, loss = 0.60328960\n","Iteration 5, loss = 0.51174351\n","Iteration 6, loss = 0.49675775\n","Iteration 7, loss = 0.45262575\n","Iteration 8, loss = 0.44856626\n","Iteration 9, loss = 0.41595844\n","Iteration 10, loss = 0.40494251\n","Iteration 11, loss = 0.40109701\n","Iteration 12, loss = 0.38828214\n","Iteration 13, loss = 0.36272718\n","Iteration 14, loss = 0.36458674\n","Iteration 15, loss = 0.35954921\n","Iteration 16, loss = 0.37561639\n","Iteration 17, loss = 0.39802785\n","Iteration 18, loss = 0.36459064\n","Iteration 19, loss = 0.38099535\n","Iteration 20, loss = 0.35058042\n","Iteration 21, loss = 0.35381674\n","Iteration 22, loss = 0.36584069\n","Iteration 23, loss = 0.37091452\n","Iteration 24, loss = 0.38089635\n","Iteration 25, loss = 0.38227790\n","Iteration 26, loss = 0.41679160\n","Iteration 27, loss = 0.34710257\n","Iteration 28, loss = 0.35182698\n","Iteration 29, loss = 0.37518700\n","Iteration 30, loss = 0.37521713\n","Iteration 31, loss = 0.36944023\n","Iteration 32, loss = 0.36187372\n","Iteration 33, loss = 0.35066864\n","Iteration 34, loss = 0.35679228\n","Iteration 35, loss = 0.35345024\n","Iteration 36, loss = 0.34917566\n","Iteration 37, loss = 0.34704890\n","Iteration 38, loss = 0.34684717\n","Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 1.27406838\n","Iteration 2, loss = 0.95132978\n","Iteration 3, loss = 0.80280396\n","Iteration 4, loss = 0.60355281\n","Iteration 5, loss = 0.49116783\n","Iteration 6, loss = 0.46103550\n","Iteration 7, loss = 0.40502698\n","Iteration 8, loss = 0.40613220\n","Iteration 9, loss = 0.44862983\n","Iteration 10, loss = 0.38266789\n","Iteration 11, loss = 0.37669266\n","Iteration 12, loss = 0.37493618\n","Iteration 13, loss = 0.34664926\n","Iteration 14, loss = 0.35563642\n","Iteration 15, loss = 0.36399869\n","Iteration 16, loss = 0.35138314\n","Iteration 17, loss = 0.36245254\n","Iteration 18, loss = 0.34581688\n","Iteration 19, loss = 0.36451421\n","Iteration 20, loss = 0.35919800\n","Iteration 21, loss = 0.33698513\n","Iteration 22, loss = 0.36536171\n","Iteration 23, loss = 0.36363696\n","Iteration 24, loss = 0.33290912\n","Iteration 25, loss = 0.38632634\n","Iteration 26, loss = 0.35908590\n","Iteration 27, loss = 0.36808194\n","Iteration 28, loss = 0.33713794\n","Iteration 29, loss = 0.39064029\n","Iteration 30, loss = 0.35480491\n","Iteration 31, loss = 0.33156189\n","Iteration 32, loss = 0.32919279\n","Iteration 33, loss = 0.33218230\n","Iteration 34, loss = 0.33597903\n","Iteration 35, loss = 0.34423460\n","Iteration 36, loss = 0.32331045\n","Iteration 37, loss = 0.33821393\n","Iteration 38, loss = 0.32890577\n","Iteration 39, loss = 0.32793327\n","Iteration 40, loss = 0.33768821\n","Iteration 41, loss = 0.32376752\n","Iteration 42, loss = 0.32172797\n","Iteration 43, loss = 0.36803742\n","Iteration 44, loss = 0.33101704\n","Iteration 45, loss = 0.34227571\n","Iteration 46, loss = 0.33103784\n","Iteration 47, loss = 0.32746238\n","Iteration 48, loss = 0.35529660\n","Iteration 49, loss = 0.31773707\n","Iteration 50, loss = 0.31596672\n","Iteration 51, loss = 0.34807164\n","Iteration 52, loss = 0.33039981\n","Iteration 53, loss = 0.31622055\n","Iteration 54, loss = 0.34274540\n","Iteration 55, loss = 0.32393608\n","Iteration 56, loss = 0.36978379\n","Iteration 57, loss = 0.31642639\n","Iteration 58, loss = 0.32749650\n","Iteration 59, loss = 0.31963816\n","Iteration 60, loss = 0.34239594\n","Iteration 61, loss = 0.31827071\n","Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 1.27321006\n","Iteration 2, loss = 0.98141194\n","Iteration 3, loss = 0.87308301\n","Iteration 4, loss = 0.59626739\n","Iteration 5, loss = 0.50246453\n","Iteration 6, loss = 0.47895163\n","Iteration 7, loss = 0.43969891\n","Iteration 8, loss = 0.41008407\n","Iteration 9, loss = 0.41046453\n","Iteration 10, loss = 0.41604849\n","Iteration 11, loss = 0.40510027\n","Iteration 12, loss = 0.46236827\n","Iteration 13, loss = 0.37439941\n","Iteration 14, loss = 0.38451891\n","Iteration 15, loss = 0.39568816\n","Iteration 16, loss = 0.37637104\n","Iteration 17, loss = 0.39440034\n","Iteration 18, loss = 0.37944470\n","Iteration 19, loss = 0.36708987\n","Iteration 20, loss = 0.36005266\n","Iteration 21, loss = 0.39656123\n","Iteration 22, loss = 0.36990721\n","Iteration 23, loss = 0.36624215\n","Iteration 24, loss = 0.35756646\n","Iteration 25, loss = 0.36265529\n","Iteration 26, loss = 0.36598109\n","Iteration 27, loss = 0.41131801\n","Iteration 28, loss = 0.36759967\n","Iteration 29, loss = 0.36888159\n","Iteration 30, loss = 0.35039261\n","Iteration 31, loss = 0.37961265\n","Iteration 32, loss = 0.36573648\n","Iteration 33, loss = 0.33445775\n","Iteration 34, loss = 0.33493403\n","Iteration 35, loss = 0.38914638\n","Iteration 36, loss = 0.33735531\n","Iteration 37, loss = 0.34365570\n","Iteration 38, loss = 0.37187469\n","Iteration 39, loss = 0.34040950\n","Iteration 40, loss = 0.34069219\n","Iteration 41, loss = 0.34292580\n","Iteration 42, loss = 0.36655465\n","Iteration 43, loss = 0.34670195\n","Iteration 44, loss = 0.31702644\n","Iteration 45, loss = 0.35598865\n","Iteration 46, loss = 0.34662958\n","Iteration 47, loss = 0.32517463\n","Iteration 48, loss = 0.37940943\n","Iteration 49, loss = 0.33833103\n","Iteration 50, loss = 0.33874099\n","Iteration 51, loss = 0.36314866\n","Iteration 52, loss = 0.33947234\n","Iteration 53, loss = 0.31573109\n","Iteration 54, loss = 0.31017365\n","Iteration 55, loss = 0.36190727\n","Iteration 56, loss = 0.33854446\n","Iteration 57, loss = 0.34916757\n","Iteration 58, loss = 0.37091135\n","Iteration 59, loss = 0.34896843\n","Iteration 60, loss = 0.34288203\n","Iteration 61, loss = 0.36541283\n","Iteration 62, loss = 0.35206980\n","Iteration 63, loss = 0.33062139\n","Iteration 64, loss = 0.39487801\n","Iteration 65, loss = 0.32851157\n","Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 1.26429330\n","Iteration 2, loss = 0.94998099\n","Iteration 3, loss = 0.77798132\n","Iteration 4, loss = 0.53562328\n","Iteration 5, loss = 0.46362954\n","Iteration 6, loss = 0.43461029\n","Iteration 7, loss = 0.38197177\n","Iteration 8, loss = 0.39693875\n","Iteration 9, loss = 0.37195236\n","Iteration 10, loss = 0.36360361\n","Iteration 11, loss = 0.37188092\n","Iteration 12, loss = 0.38569649\n","Iteration 13, loss = 0.35410571\n","Iteration 14, loss = 0.36782415\n","Iteration 15, loss = 0.35829148\n","Iteration 16, loss = 0.34224058\n","Iteration 17, loss = 0.36218224\n","Iteration 18, loss = 0.33247191\n","Iteration 19, loss = 0.38750414\n","Iteration 20, loss = 0.35773573\n","Iteration 21, loss = 0.33635653\n","Iteration 22, loss = 0.36803233\n","Iteration 23, loss = 0.39870301\n","Iteration 24, loss = 0.34316843\n","Iteration 25, loss = 0.37276679\n","Iteration 26, loss = 0.35295040\n","Iteration 27, loss = 0.32763917\n","Iteration 28, loss = 0.35769809\n","Iteration 29, loss = 0.38166263\n","Iteration 30, loss = 0.35317321\n","Iteration 31, loss = 0.33726154\n","Iteration 32, loss = 0.34954728\n","Iteration 33, loss = 0.42821987\n","Iteration 34, loss = 0.32166463\n","Iteration 35, loss = 0.36470650\n","Iteration 36, loss = 0.32379619\n","Iteration 37, loss = 0.35180765\n","Iteration 38, loss = 0.32444553\n","Iteration 39, loss = 0.33715645\n","Iteration 40, loss = 0.31842463\n","Iteration 41, loss = 0.33108846\n","Iteration 42, loss = 0.34540065\n","Iteration 43, loss = 0.33191206\n","Iteration 44, loss = 0.32504624\n","Iteration 45, loss = 0.34798710\n","Iteration 46, loss = 0.32932054\n","Iteration 47, loss = 0.34310994\n","Iteration 48, loss = 0.34750152\n","Iteration 49, loss = 0.34311050\n","Iteration 50, loss = 0.33357437\n","Iteration 51, loss = 0.35459299\n","Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 1.22192381\n","Iteration 2, loss = 0.88327612\n","Iteration 3, loss = 0.80142100\n","Iteration 4, loss = 0.74820692\n","Iteration 5, loss = 0.59136731\n","Iteration 6, loss = 0.45677030\n","Iteration 7, loss = 0.44219920\n","Iteration 8, loss = 0.39246526\n","Iteration 9, loss = 0.36439405\n","Iteration 10, loss = 0.41010331\n","Iteration 11, loss = 0.38097279\n","Iteration 12, loss = 0.40286681\n","Iteration 13, loss = 0.33401497\n","Iteration 14, loss = 0.33310465\n","Iteration 15, loss = 0.30171279\n","Iteration 16, loss = 0.35092987\n","Iteration 17, loss = 0.31783710\n","Iteration 18, loss = 0.29336881\n","Iteration 19, loss = 0.33166792\n","Iteration 20, loss = 0.30559531\n","Iteration 21, loss = 0.33008098\n","Iteration 22, loss = 0.34531540\n","Iteration 23, loss = 0.27487608\n","Iteration 24, loss = 0.27158332\n","Iteration 25, loss = 0.30098439\n","Iteration 26, loss = 0.32270889\n","Iteration 27, loss = 0.29637019\n","Iteration 28, loss = 0.27174990\n","Iteration 29, loss = 0.26420497\n","Iteration 30, loss = 0.30810594\n","Iteration 31, loss = 0.26035960\n","Iteration 32, loss = 0.28563310\n","Iteration 33, loss = 0.25806289\n","Iteration 34, loss = 0.25915382\n","Iteration 35, loss = 0.25095109\n","Iteration 36, loss = 0.25494602\n","Iteration 37, loss = 0.35053094\n","Iteration 38, loss = 0.27768051\n","Iteration 39, loss = 0.25861126\n","Iteration 40, loss = 0.27202421\n","Iteration 41, loss = 0.30992472\n","Iteration 42, loss = 0.26945366\n","Iteration 43, loss = 0.24962722\n","Iteration 44, loss = 0.24630763\n","Iteration 45, loss = 0.29445174\n","Iteration 46, loss = 0.30906920\n","Iteration 47, loss = 0.27610088\n","Iteration 48, loss = 0.29309787\n","Iteration 49, loss = 0.24774376\n","Iteration 50, loss = 0.25386436\n","Iteration 51, loss = 0.26267197\n","Iteration 52, loss = 0.25271739\n","Iteration 53, loss = 0.25953172\n","Iteration 54, loss = 0.26539442\n","Iteration 55, loss = 0.26724490\n","Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n","Experimento 2\n","Acurácia Média: 86.79%\n","Precisão Média: 89.59%\n","Revocação Média: 87.86%\n","F1-Score Médio: 87.57%\n"]}],"source":["from sklearn.neural_network import MLPClassifier\n","from sklearn.model_selection import cross_validate\n","\n","\n","x_train = df_scaled.iloc[:, 0:16]\n","y_train = df_scaled.iloc[:, 16]\n","classifier = MLPClassifier(activation='logistic', solver='adam', alpha=1e-5, hidden_layer_sizes=(12, 3), random_state=1, verbose=True, learning_rate_init=0.3, tol=1e-3, max_iter=500)\n","scoring = {'acc' : 'accuracy',\n","           'prec' : 'precision_macro',\n","           'recall' : 'recall_macro',\n","           'f1' : 'f1_macro'}\n","\n","\n","y_pred = cross_validate(classifier, x_train, y_train, cv=10, scoring=scoring, return_train_score=True)\n","print('Experimento 2')\n","print('Acurácia Média: ' + '%.2f' % (np.mean(y_pred['test_acc'])*100) + '%')\n","print('Precisão Média: ' + '%.2f' % (np.mean(y_pred['test_prec'])*100) + '%')\n","print('Revocação Média: ' + '%.2f' % (np.mean(y_pred['test_recall'])*100) + '%')\n","print('F1-Score Médio: ' + '%.2f' % (np.mean(y_pred['test_f1'])*100) + '%')\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOcxPcBSxBGnyxp1DZzHnhO","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}

{"cells":[{"cell_type":"markdown","metadata":{"id":"VMpZC5eq6HNR"},"source":["Experimento 16 - Linear Interpolation Imputation, MAD outlier detection, Z-Score normalization, MLP classifier"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":18342,"status":"ok","timestamp":1704302942662,"user":{"displayName":"Victor Hugo Schneider Lopes","userId":"10992764016890176856"},"user_tz":180},"id":"cP53TLVS5xq-"},"outputs":[],"source":["import pandas as pd\n","import seaborn as sns\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn.objects as so\n","from ucimlrepo import fetch_ucirepo"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":1016,"status":"ok","timestamp":1704302943661,"user":{"displayName":"Victor Hugo Schneider Lopes","userId":"10992764016890176856"},"user_tz":180},"id":"0owuQnJm6Vl5"},"outputs":[],"source":["beans = fetch_ucirepo(id=602)\n","df = beans.data.features\n","targets = beans.data.targets"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1704302943661,"user":{"displayName":"Victor Hugo Schneider Lopes","userId":"10992764016890176856"},"user_tz":180},"id":"_jMLf_dB6aIn"},"outputs":[],"source":["cols = ['Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength', 'AspectRatio', 'Eccentricity', 'ConvexArea', 'EquivDiameter', 'Extent', 'Solidity', 'Roundness', 'Compactness', 'ShapeFactor1', 'ShapeFactor2', 'ShapeFactor3', 'ShapeFactor4']"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1704302947554,"user":{"displayName":"Victor Hugo Schneider Lopes","userId":"10992764016890176856"},"user_tz":180},"id":"dsT5e4MCtxV0","outputId":"45d7b101-70b8-45a7-bcaa-cfb3a102f067"},"outputs":[{"name":"stdout","output_type":"stream","text":["          Area Perimeter MajorAxisLength MinorAxisLength AspectRatio  \\\n","         count     count           count           count       count   \n","Class                                                                  \n","BARBUNYA  1322      1322            1322            1322        1322   \n","BOMBAY     522       522             522             522         522   \n","CALI      1630      1630            1630            1630        1630   \n","DERMASON  3546      3546            3546            3546        3546   \n","HOROZ     1928      1928            1928            1928        1928   \n","SEKER     2027      2027            2027            2027        2027   \n","SIRA      2636      2636            2636            2636        2636   \n","\n","         Eccentricity ConvexArea EquivDiameter Extent Solidity Roundness  \\\n","                count      count         count  count    count     count   \n","Class                                                                      \n","BARBUNYA         1322       1322          1322   1322     1322      1322   \n","BOMBAY            522        522           522    522      522       522   \n","CALI             1630       1630          1630   1630     1630      1630   \n","DERMASON         3546       3546          3546   3546     3546      3546   \n","HOROZ            1928       1928          1928   1928     1928      1928   \n","SEKER            2027       2027          2027   2027     2027      2027   \n","SIRA             2636       2636          2636   2636     2636      2636   \n","\n","         Compactness ShapeFactor1 ShapeFactor2 ShapeFactor3 ShapeFactor4  \n","               count        count        count        count        count  \n","Class                                                                     \n","BARBUNYA        1322         1322         1322         1322         1322  \n","BOMBAY           522          522          522          522          522  \n","CALI            1630         1630         1630         1630         1630  \n","DERMASON        3546         3546         3546         3546         3546  \n","HOROZ           1928         1928         1928         1928         1928  \n","SEKER           2027         2027         2027         2027         2027  \n","SIRA            2636         2636         2636         2636         2636  \n"]}],"source":["#adding the labels\n","df['Class'] = targets\n","df_pre_missing_values = df.copy()\n","print(df.groupby('Class').agg(['count']))\n","df['Class'] = df['Class'].transform(lambda x: 0 if x == 'BARBUNYA' else (1 if x == 'BOMBAY' else (2 if x == 'CALI' else (3 if x == 'DERMASON' else (4 if x == 'HOROZ' else (5 if x == 'SEKER' else 6))))))"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1573,"status":"ok","timestamp":1704302949104,"user":{"displayName":"Victor Hugo Schneider Lopes","userId":"10992764016890176856"},"user_tz":180},"id":"ML8h0BGivuxc","outputId":"df7849d9-1a7c-4e7a-8b90-e90a57e99139"},"outputs":[{"name":"stdout","output_type":"stream","text":["Pre Outlier Shape: (13611, 17)\n","[5 0 1 2 4 6 3]\n","Pos Outlier Shape: (12198, 17)\n"]}],"source":["#Outlier Removal with MAD\n","from scipy import stats\n","\n","print(f'Pre Outlier Shape: {df.shape}')\n","\n","df_no_outliers = df.copy()\n","print(df_no_outliers['Class'].unique())\n","for i in df_no_outliers['Class'].unique():\n","    class_unique = df_no_outliers[df_no_outliers['Class'] == i]\n","    for feature in class_unique:\n","      mad = 1.4826 * np.median(np.absolute(class_unique[feature] - class_unique[feature].median()))\n","      #print(mad)\n","      upper = class_unique[feature].median() + (3 * mad)\n","      lower = class_unique[feature].median() - (3 * mad)\n","      excluded_lower = pd.Series(class_unique[class_unique[feature] < lower].index)\n","      excluded_upper = pd.Series(class_unique[class_unique[feature] > upper].index)\n","      df_no_outliers.drop(excluded_lower.values, inplace = True, errors='ignore')\n","      df_no_outliers.drop(excluded_upper.values, inplace = True, errors='ignore')\n","\n","\n","print(f'Pos Outlier Shape: {df_no_outliers.shape}')"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40,"status":"ok","timestamp":1704302949105,"user":{"displayName":"Victor Hugo Schneider Lopes","userId":"10992764016890176856"},"user_tz":180},"id":"LlYPv-ra64um","outputId":"9c467a89-8e02-46cc-a41b-0ef47c1a8ec7"},"outputs":[{"name":"stdout","output_type":"stream","text":["         index  Area Perimeter MajorAxisLength MinorAxisLength AspectRatio  \\\n","         count count     count           count           count       count   \n","Class                                                                        \n","BARBUNYA  1193  1193      1193            1193            1193        1193   \n","BOMBAY     472   472       472             472             472         472   \n","CALI      1512  1512      1512            1512            1512        1512   \n","DERMASON  3215  3215      3215            3215            3215        3215   \n","HOROZ     1641  1641      1641            1641            1641        1641   \n","SEKER     1762  1762      1762            1762            1762        1762   \n","SIRA      2403  2403      2403            2403            2403        2403   \n","\n","         Eccentricity ConvexArea EquivDiameter Extent Solidity Roundness  \\\n","                count      count         count  count    count     count   \n","Class                                                                      \n","BARBUNYA         1193       1193          1193   1193     1193      1193   \n","BOMBAY            472        472           472    472      472       472   \n","CALI             1512       1512          1512   1512     1512      1512   \n","DERMASON         3215       3215          3215   3215     3215      3215   \n","HOROZ            1641       1641          1641   1641     1641      1641   \n","SEKER            1762       1762          1762   1762     1762      1762   \n","SIRA             2403       2403          2403   2403     2403      2403   \n","\n","         Compactness ShapeFactor1 ShapeFactor2 ShapeFactor3 ShapeFactor4  \n","               count        count        count        count        count  \n","Class                                                                     \n","BARBUNYA        1193         1193         1193         1193         1193  \n","BOMBAY           472          472          472          472          472  \n","CALI            1512         1512         1512         1512         1512  \n","DERMASON        3215         3215         3215         3215         3215  \n","HOROZ           1641         1641         1641         1641         1641  \n","SEKER           1762         1762         1762         1762         1762  \n","SIRA            2403         2403         2403         2403         2403  \n"]}],"source":["df_no_outliers['Class'] = df_no_outliers['Class'].transform(lambda x: 'BARBUNYA' if x == 0 else ('BOMBAY' if x == 1 else ('CALI' if x == 2 else ('DERMASON' if x == 3 else ('HOROZ' if x == 4 else ('SEKER' if x == 5 else 'SIRA'))))))\n","df_no_outliers = df_no_outliers.reset_index()\n","print(df_no_outliers.groupby('Class').agg(['count']))"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":444},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1704302949106,"user":{"displayName":"Victor Hugo Schneider Lopes","userId":"10992764016890176856"},"user_tz":180},"id":"T8INyItCFB8j","outputId":"aba6385f-9c54-4e3c-d2c1-1f28015a69f0"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Area</th>\n","      <th>Perimeter</th>\n","      <th>MajorAxisLength</th>\n","      <th>MinorAxisLength</th>\n","      <th>AspectRatio</th>\n","      <th>Eccentricity</th>\n","      <th>ConvexArea</th>\n","      <th>EquivDiameter</th>\n","      <th>Extent</th>\n","      <th>Solidity</th>\n","      <th>Roundness</th>\n","      <th>Compactness</th>\n","      <th>ShapeFactor1</th>\n","      <th>ShapeFactor2</th>\n","      <th>ShapeFactor3</th>\n","      <th>ShapeFactor4</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>28395</td>\n","      <td>610.291</td>\n","      <td>208.178117</td>\n","      <td>173.888747</td>\n","      <td>1.197191</td>\n","      <td>0.549812</td>\n","      <td>28715</td>\n","      <td>190.141097</td>\n","      <td>0.763923</td>\n","      <td>0.988856</td>\n","      <td>0.958027</td>\n","      <td>0.913358</td>\n","      <td>0.007332</td>\n","      <td>0.003147</td>\n","      <td>0.834222</td>\n","      <td>0.998724</td>\n","      <td>SEKER</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>29380</td>\n","      <td>624.110</td>\n","      <td>212.826130</td>\n","      <td>175.931143</td>\n","      <td>1.209713</td>\n","      <td>0.562727</td>\n","      <td>29690</td>\n","      <td>193.410904</td>\n","      <td>0.778113</td>\n","      <td>0.989559</td>\n","      <td>0.947849</td>\n","      <td>0.908774</td>\n","      <td>0.007244</td>\n","      <td>0.003048</td>\n","      <td>0.825871</td>\n","      <td>0.999066</td>\n","      <td>SEKER</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>30279</td>\n","      <td>634.927</td>\n","      <td>212.560556</td>\n","      <td>181.510182</td>\n","      <td>1.171067</td>\n","      <td>0.520401</td>\n","      <td>30600</td>\n","      <td>196.347702</td>\n","      <td>0.775688</td>\n","      <td>0.989510</td>\n","      <td>0.943852</td>\n","      <td>0.923726</td>\n","      <td>0.007020</td>\n","      <td>0.003153</td>\n","      <td>0.853270</td>\n","      <td>0.999236</td>\n","      <td>SEKER</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>30519</td>\n","      <td>629.727</td>\n","      <td>212.996755</td>\n","      <td>182.737204</td>\n","      <td>1.165591</td>\n","      <td>0.513760</td>\n","      <td>30847</td>\n","      <td>197.124320</td>\n","      <td>0.770682</td>\n","      <td>0.989367</td>\n","      <td>0.967109</td>\n","      <td>0.925480</td>\n","      <td>0.006979</td>\n","      <td>0.003158</td>\n","      <td>0.856514</td>\n","      <td>0.998345</td>\n","      <td>SEKER</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>30685</td>\n","      <td>635.681</td>\n","      <td>213.534145</td>\n","      <td>183.157146</td>\n","      <td>1.165852</td>\n","      <td>0.514081</td>\n","      <td>31044</td>\n","      <td>197.659696</td>\n","      <td>0.771561</td>\n","      <td>0.988436</td>\n","      <td>0.954240</td>\n","      <td>0.925658</td>\n","      <td>0.006959</td>\n","      <td>0.003152</td>\n","      <td>0.856844</td>\n","      <td>0.998953</td>\n","      <td>SEKER</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>12193</th>\n","      <td>42097</td>\n","      <td>759.696</td>\n","      <td>288.721612</td>\n","      <td>185.944705</td>\n","      <td>1.552728</td>\n","      <td>0.765002</td>\n","      <td>42508</td>\n","      <td>231.515799</td>\n","      <td>0.714574</td>\n","      <td>0.990331</td>\n","      <td>0.916603</td>\n","      <td>0.801865</td>\n","      <td>0.006858</td>\n","      <td>0.001749</td>\n","      <td>0.642988</td>\n","      <td>0.998385</td>\n","      <td>DERMASON</td>\n","    </tr>\n","    <tr>\n","      <th>12194</th>\n","      <td>42101</td>\n","      <td>757.499</td>\n","      <td>281.576392</td>\n","      <td>190.713136</td>\n","      <td>1.476439</td>\n","      <td>0.735702</td>\n","      <td>42494</td>\n","      <td>231.526798</td>\n","      <td>0.799943</td>\n","      <td>0.990752</td>\n","      <td>0.922015</td>\n","      <td>0.822252</td>\n","      <td>0.006688</td>\n","      <td>0.001886</td>\n","      <td>0.676099</td>\n","      <td>0.998219</td>\n","      <td>DERMASON</td>\n","    </tr>\n","    <tr>\n","      <th>12195</th>\n","      <td>42139</td>\n","      <td>759.321</td>\n","      <td>281.539928</td>\n","      <td>191.187979</td>\n","      <td>1.472582</td>\n","      <td>0.734065</td>\n","      <td>42569</td>\n","      <td>231.631261</td>\n","      <td>0.729932</td>\n","      <td>0.989899</td>\n","      <td>0.918424</td>\n","      <td>0.822730</td>\n","      <td>0.006681</td>\n","      <td>0.001888</td>\n","      <td>0.676884</td>\n","      <td>0.996767</td>\n","      <td>DERMASON</td>\n","    </tr>\n","    <tr>\n","      <th>12196</th>\n","      <td>42147</td>\n","      <td>763.779</td>\n","      <td>283.382636</td>\n","      <td>190.275731</td>\n","      <td>1.489326</td>\n","      <td>0.741055</td>\n","      <td>42667</td>\n","      <td>231.653247</td>\n","      <td>0.705389</td>\n","      <td>0.987813</td>\n","      <td>0.907906</td>\n","      <td>0.817457</td>\n","      <td>0.006724</td>\n","      <td>0.001852</td>\n","      <td>0.668237</td>\n","      <td>0.995222</td>\n","      <td>DERMASON</td>\n","    </tr>\n","    <tr>\n","      <th>12197</th>\n","      <td>42159</td>\n","      <td>772.237</td>\n","      <td>295.142741</td>\n","      <td>182.204716</td>\n","      <td>1.619841</td>\n","      <td>0.786693</td>\n","      <td>42600</td>\n","      <td>231.686223</td>\n","      <td>0.788962</td>\n","      <td>0.989648</td>\n","      <td>0.888380</td>\n","      <td>0.784997</td>\n","      <td>0.007001</td>\n","      <td>0.001640</td>\n","      <td>0.616221</td>\n","      <td>0.998180</td>\n","      <td>DERMASON</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>12198 rows × 17 columns</p>\n","</div>"],"text/plain":["        Area  Perimeter  MajorAxisLength  MinorAxisLength  AspectRatio  \\\n","0      28395    610.291       208.178117       173.888747     1.197191   \n","1      29380    624.110       212.826130       175.931143     1.209713   \n","2      30279    634.927       212.560556       181.510182     1.171067   \n","3      30519    629.727       212.996755       182.737204     1.165591   \n","4      30685    635.681       213.534145       183.157146     1.165852   \n","...      ...        ...              ...              ...          ...   \n","12193  42097    759.696       288.721612       185.944705     1.552728   \n","12194  42101    757.499       281.576392       190.713136     1.476439   \n","12195  42139    759.321       281.539928       191.187979     1.472582   \n","12196  42147    763.779       283.382636       190.275731     1.489326   \n","12197  42159    772.237       295.142741       182.204716     1.619841   \n","\n","       Eccentricity  ConvexArea  EquivDiameter    Extent  Solidity  Roundness  \\\n","0          0.549812       28715     190.141097  0.763923  0.988856   0.958027   \n","1          0.562727       29690     193.410904  0.778113  0.989559   0.947849   \n","2          0.520401       30600     196.347702  0.775688  0.989510   0.943852   \n","3          0.513760       30847     197.124320  0.770682  0.989367   0.967109   \n","4          0.514081       31044     197.659696  0.771561  0.988436   0.954240   \n","...             ...         ...            ...       ...       ...        ...   \n","12193      0.765002       42508     231.515799  0.714574  0.990331   0.916603   \n","12194      0.735702       42494     231.526798  0.799943  0.990752   0.922015   \n","12195      0.734065       42569     231.631261  0.729932  0.989899   0.918424   \n","12196      0.741055       42667     231.653247  0.705389  0.987813   0.907906   \n","12197      0.786693       42600     231.686223  0.788962  0.989648   0.888380   \n","\n","       Compactness  ShapeFactor1  ShapeFactor2  ShapeFactor3  ShapeFactor4  \\\n","0         0.913358      0.007332      0.003147      0.834222      0.998724   \n","1         0.908774      0.007244      0.003048      0.825871      0.999066   \n","2         0.923726      0.007020      0.003153      0.853270      0.999236   \n","3         0.925480      0.006979      0.003158      0.856514      0.998345   \n","4         0.925658      0.006959      0.003152      0.856844      0.998953   \n","...            ...           ...           ...           ...           ...   \n","12193     0.801865      0.006858      0.001749      0.642988      0.998385   \n","12194     0.822252      0.006688      0.001886      0.676099      0.998219   \n","12195     0.822730      0.006681      0.001888      0.676884      0.996767   \n","12196     0.817457      0.006724      0.001852      0.668237      0.995222   \n","12197     0.784997      0.007001      0.001640      0.616221      0.998180   \n","\n","          Class  \n","0         SEKER  \n","1         SEKER  \n","2         SEKER  \n","3         SEKER  \n","4         SEKER  \n","...         ...  \n","12193  DERMASON  \n","12194  DERMASON  \n","12195  DERMASON  \n","12196  DERMASON  \n","12197  DERMASON  \n","\n","[12198 rows x 17 columns]"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["df_no_outliers = df_no_outliers.drop(['index'], axis='columns')\n","df_no_outliers"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":444},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1704302949107,"user":{"displayName":"Victor Hugo Schneider Lopes","userId":"10992764016890176856"},"user_tz":180},"id":"nSe3jkEjynF6","outputId":"b8fd8100-7e7e-4761-bd09-854f0e7fc717"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Area</th>\n","      <th>Perimeter</th>\n","      <th>MajorAxisLength</th>\n","      <th>MinorAxisLength</th>\n","      <th>AspectRatio</th>\n","      <th>Eccentricity</th>\n","      <th>ConvexArea</th>\n","      <th>EquivDiameter</th>\n","      <th>Extent</th>\n","      <th>Solidity</th>\n","      <th>Roundness</th>\n","      <th>Compactness</th>\n","      <th>ShapeFactor1</th>\n","      <th>ShapeFactor2</th>\n","      <th>ShapeFactor3</th>\n","      <th>ShapeFactor4</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.849054</td>\n","      <td>-1.145239</td>\n","      <td>-1.315645</td>\n","      <td>-0.638833</td>\n","      <td>-1.576380</td>\n","      <td>-2.251665</td>\n","      <td>-0.849310</td>\n","      <td>-1.072789</td>\n","      <td>0.264907</td>\n","      <td>0.304790</td>\n","      <td>1.428298</td>\n","      <td>1.863735</td>\n","      <td>0.700289</td>\n","      <td>2.430380</td>\n","      <td>1.955690</td>\n","      <td>0.954328</td>\n","      <td>SEKER</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.815319</td>\n","      <td>-1.080532</td>\n","      <td>-1.261260</td>\n","      <td>-0.593146</td>\n","      <td>-1.525318</td>\n","      <td>-2.107666</td>\n","      <td>-0.816390</td>\n","      <td>-1.017380</td>\n","      <td>0.556502</td>\n","      <td>0.507774</td>\n","      <td>1.249673</td>\n","      <td>1.788499</td>\n","      <td>0.621722</td>\n","      <td>2.261789</td>\n","      <td>1.870087</td>\n","      <td>1.058474</td>\n","      <td>SEKER</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.784529</td>\n","      <td>-1.029882</td>\n","      <td>-1.264367</td>\n","      <td>-0.468347</td>\n","      <td>-1.682918</td>\n","      <td>-2.579593</td>\n","      <td>-0.785665</td>\n","      <td>-0.967613</td>\n","      <td>0.506677</td>\n","      <td>0.493630</td>\n","      <td>1.179511</td>\n","      <td>2.033924</td>\n","      <td>0.420945</td>\n","      <td>2.439676</td>\n","      <td>2.150917</td>\n","      <td>1.110096</td>\n","      <td>SEKER</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-0.776309</td>\n","      <td>-1.054231</td>\n","      <td>-1.259263</td>\n","      <td>-0.440900</td>\n","      <td>-1.705251</td>\n","      <td>-2.653638</td>\n","      <td>-0.777325</td>\n","      <td>-0.954453</td>\n","      <td>0.403799</td>\n","      <td>0.452347</td>\n","      <td>1.587695</td>\n","      <td>2.062722</td>\n","      <td>0.384248</td>\n","      <td>2.449000</td>\n","      <td>2.184170</td>\n","      <td>0.838899</td>\n","      <td>SEKER</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.770624</td>\n","      <td>-1.026352</td>\n","      <td>-1.252976</td>\n","      <td>-0.431506</td>\n","      <td>-1.704185</td>\n","      <td>-2.650056</td>\n","      <td>-0.770674</td>\n","      <td>-0.945381</td>\n","      <td>0.421875</td>\n","      <td>0.183415</td>\n","      <td>1.361828</td>\n","      <td>2.065645</td>\n","      <td>0.366092</td>\n","      <td>2.437595</td>\n","      <td>2.187549</td>\n","      <td>1.024040</td>\n","      <td>SEKER</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>12193</th>\n","      <td>-0.379778</td>\n","      <td>-0.445659</td>\n","      <td>-0.373231</td>\n","      <td>-0.369150</td>\n","      <td>-0.126473</td>\n","      <td>0.147623</td>\n","      <td>-0.383603</td>\n","      <td>-0.371660</td>\n","      <td>-0.749112</td>\n","      <td>0.730884</td>\n","      <td>0.701278</td>\n","      <td>0.033645</td>\n","      <td>0.276016</td>\n","      <td>0.062648</td>\n","      <td>-0.004392</td>\n","      <td>0.851280</td>\n","      <td>DERMASON</td>\n","    </tr>\n","    <tr>\n","      <th>12194</th>\n","      <td>-0.379641</td>\n","      <td>-0.455946</td>\n","      <td>-0.456835</td>\n","      <td>-0.262484</td>\n","      <td>-0.437585</td>\n","      <td>-0.179061</td>\n","      <td>-0.384076</td>\n","      <td>-0.371474</td>\n","      <td>1.005065</td>\n","      <td>0.852309</td>\n","      <td>0.796266</td>\n","      <td>0.368287</td>\n","      <td>0.123207</td>\n","      <td>0.294208</td>\n","      <td>0.334982</td>\n","      <td>0.800586</td>\n","      <td>DERMASON</td>\n","    </tr>\n","    <tr>\n","      <th>12195</th>\n","      <td>-0.378339</td>\n","      <td>-0.447415</td>\n","      <td>-0.457262</td>\n","      <td>-0.251862</td>\n","      <td>-0.453317</td>\n","      <td>-0.197317</td>\n","      <td>-0.381544</td>\n","      <td>-0.369704</td>\n","      <td>-0.433529</td>\n","      <td>0.605970</td>\n","      <td>0.733237</td>\n","      <td>0.376125</td>\n","      <td>0.117021</td>\n","      <td>0.298333</td>\n","      <td>0.343033</td>\n","      <td>0.358930</td>\n","      <td>DERMASON</td>\n","    </tr>\n","    <tr>\n","      <th>12196</th>\n","      <td>-0.378065</td>\n","      <td>-0.426540</td>\n","      <td>-0.435701</td>\n","      <td>-0.272268</td>\n","      <td>-0.385032</td>\n","      <td>-0.119381</td>\n","      <td>-0.378235</td>\n","      <td>-0.369331</td>\n","      <td>-0.937851</td>\n","      <td>0.003423</td>\n","      <td>0.548645</td>\n","      <td>0.289584</td>\n","      <td>0.155099</td>\n","      <td>0.236954</td>\n","      <td>0.254400</td>\n","      <td>-0.111163</td>\n","      <td>DERMASON</td>\n","    </tr>\n","    <tr>\n","      <th>12197</th>\n","      <td>-0.377654</td>\n","      <td>-0.386936</td>\n","      <td>-0.298099</td>\n","      <td>-0.452811</td>\n","      <td>0.147220</td>\n","      <td>0.389468</td>\n","      <td>-0.380497</td>\n","      <td>-0.368772</td>\n","      <td>0.779435</td>\n","      <td>0.533512</td>\n","      <td>0.205949</td>\n","      <td>-0.243234</td>\n","      <td>0.403580</td>\n","      <td>-0.122412</td>\n","      <td>-0.278745</td>\n","      <td>0.788709</td>\n","      <td>DERMASON</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>12198 rows × 17 columns</p>\n","</div>"],"text/plain":["           Area  Perimeter  MajorAxisLength  MinorAxisLength  AspectRatio  \\\n","0     -0.849054  -1.145239        -1.315645        -0.638833    -1.576380   \n","1     -0.815319  -1.080532        -1.261260        -0.593146    -1.525318   \n","2     -0.784529  -1.029882        -1.264367        -0.468347    -1.682918   \n","3     -0.776309  -1.054231        -1.259263        -0.440900    -1.705251   \n","4     -0.770624  -1.026352        -1.252976        -0.431506    -1.704185   \n","...         ...        ...              ...              ...          ...   \n","12193 -0.379778  -0.445659        -0.373231        -0.369150    -0.126473   \n","12194 -0.379641  -0.455946        -0.456835        -0.262484    -0.437585   \n","12195 -0.378339  -0.447415        -0.457262        -0.251862    -0.453317   \n","12196 -0.378065  -0.426540        -0.435701        -0.272268    -0.385032   \n","12197 -0.377654  -0.386936        -0.298099        -0.452811     0.147220   \n","\n","       Eccentricity  ConvexArea  EquivDiameter    Extent  Solidity  Roundness  \\\n","0         -2.251665   -0.849310      -1.072789  0.264907  0.304790   1.428298   \n","1         -2.107666   -0.816390      -1.017380  0.556502  0.507774   1.249673   \n","2         -2.579593   -0.785665      -0.967613  0.506677  0.493630   1.179511   \n","3         -2.653638   -0.777325      -0.954453  0.403799  0.452347   1.587695   \n","4         -2.650056   -0.770674      -0.945381  0.421875  0.183415   1.361828   \n","...             ...         ...            ...       ...       ...        ...   \n","12193      0.147623   -0.383603      -0.371660 -0.749112  0.730884   0.701278   \n","12194     -0.179061   -0.384076      -0.371474  1.005065  0.852309   0.796266   \n","12195     -0.197317   -0.381544      -0.369704 -0.433529  0.605970   0.733237   \n","12196     -0.119381   -0.378235      -0.369331 -0.937851  0.003423   0.548645   \n","12197      0.389468   -0.380497      -0.368772  0.779435  0.533512   0.205949   \n","\n","       Compactness  ShapeFactor1  ShapeFactor2  ShapeFactor3  ShapeFactor4  \\\n","0         1.863735      0.700289      2.430380      1.955690      0.954328   \n","1         1.788499      0.621722      2.261789      1.870087      1.058474   \n","2         2.033924      0.420945      2.439676      2.150917      1.110096   \n","3         2.062722      0.384248      2.449000      2.184170      0.838899   \n","4         2.065645      0.366092      2.437595      2.187549      1.024040   \n","...            ...           ...           ...           ...           ...   \n","12193     0.033645      0.276016      0.062648     -0.004392      0.851280   \n","12194     0.368287      0.123207      0.294208      0.334982      0.800586   \n","12195     0.376125      0.117021      0.298333      0.343033      0.358930   \n","12196     0.289584      0.155099      0.236954      0.254400     -0.111163   \n","12197    -0.243234      0.403580     -0.122412     -0.278745      0.788709   \n","\n","          Class  \n","0         SEKER  \n","1         SEKER  \n","2         SEKER  \n","3         SEKER  \n","4         SEKER  \n","...         ...  \n","12193  DERMASON  \n","12194  DERMASON  \n","12195  DERMASON  \n","12196  DERMASON  \n","12197  DERMASON  \n","\n","[12198 rows x 17 columns]"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["#Normalization with Z-Score\n","from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","scaler.fit_transform(df_no_outliers[cols])\n","df_scaled = pd.DataFrame(scaler.transform(df_no_outliers[cols]), columns = cols)\n","\n","df_scaled['Class'] = df_no_outliers['Class']\n","df_scaled"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27198,"status":"ok","timestamp":1704302976281,"user":{"displayName":"Victor Hugo Schneider Lopes","userId":"10992764016890176856"},"user_tz":180},"id":"0dBs8lW22D02","outputId":"08aefeb8-eb63-42f6-a6a7-b9e02902cca5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration 1, loss = 1.22522409\n","Iteration 2, loss = 0.78290889\n","Iteration 3, loss = 0.52785082\n","Iteration 4, loss = 0.37035421\n","Iteration 5, loss = 0.37070454\n","Iteration 6, loss = 0.35178675\n","Iteration 7, loss = 0.35324611\n","Iteration 8, loss = 0.33819123\n","Iteration 9, loss = 0.31433031\n","Iteration 10, loss = 0.30797133\n","Iteration 11, loss = 0.30500809\n","Iteration 12, loss = 0.31009914\n","Iteration 13, loss = 0.30478057\n","Iteration 14, loss = 0.30242965\n","Iteration 15, loss = 0.28623141\n","Iteration 16, loss = 0.28607353\n","Iteration 17, loss = 0.30265468\n","Iteration 18, loss = 0.29052759\n","Iteration 19, loss = 0.27291652\n","Iteration 20, loss = 0.26171572\n","Iteration 21, loss = 0.25804438\n","Iteration 22, loss = 0.21654335\n","Iteration 23, loss = 0.21085686\n","Iteration 24, loss = 0.21759954\n","Iteration 25, loss = 0.19382927\n","Iteration 26, loss = 0.17694611\n","Iteration 27, loss = 0.19403728\n","Iteration 28, loss = 0.21884553\n","Iteration 29, loss = 0.18643494\n","Iteration 30, loss = 0.16950896\n","Iteration 31, loss = 0.16518462\n","Iteration 32, loss = 0.17608011\n","Iteration 33, loss = 0.17203807\n","Iteration 34, loss = 0.16964520\n","Iteration 35, loss = 0.18871821\n","Iteration 36, loss = 0.17432501\n","Iteration 37, loss = 0.16674426\n","Iteration 38, loss = 0.18725863\n","Iteration 39, loss = 0.18085687\n","Iteration 40, loss = 0.18239063\n","Iteration 41, loss = 0.15033810\n","Iteration 42, loss = 0.14671879\n","Iteration 43, loss = 0.18037422\n","Iteration 44, loss = 0.15461349\n","Iteration 45, loss = 0.15127627\n","Iteration 46, loss = 0.16398056\n","Iteration 47, loss = 0.15654215\n","Iteration 48, loss = 0.17688722\n","Iteration 49, loss = 0.17031531\n","Iteration 50, loss = 0.16656908\n","Iteration 51, loss = 0.15955903\n","Iteration 52, loss = 0.17422894\n","Iteration 53, loss = 0.16222726\n","Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 1.22000671\n","Iteration 2, loss = 0.79073021\n","Iteration 3, loss = 0.39805504\n","Iteration 4, loss = 0.27218151\n","Iteration 5, loss = 0.26551720\n","Iteration 6, loss = 0.23543426\n","Iteration 7, loss = 0.23300813\n","Iteration 8, loss = 0.22570980\n","Iteration 9, loss = 0.21566577\n","Iteration 10, loss = 0.22898092\n","Iteration 11, loss = 0.21190280\n","Iteration 12, loss = 0.22970390\n","Iteration 13, loss = 0.21252874\n","Iteration 14, loss = 0.21376554\n","Iteration 15, loss = 0.23149828\n","Iteration 16, loss = 0.20937158\n","Iteration 17, loss = 0.21653265\n","Iteration 18, loss = 0.22739165\n","Iteration 19, loss = 0.20397927\n","Iteration 20, loss = 0.20311107\n","Iteration 21, loss = 0.19470013\n","Iteration 22, loss = 0.19295753\n","Iteration 23, loss = 0.20152269\n","Iteration 24, loss = 0.22641317\n","Iteration 25, loss = 0.22640169\n","Iteration 26, loss = 0.21932678\n","Iteration 27, loss = 0.22414315\n","Iteration 28, loss = 0.21799007\n","Iteration 29, loss = 0.21006219\n","Iteration 30, loss = 0.23363434\n","Iteration 31, loss = 0.22062283\n","Iteration 32, loss = 0.20769148\n","Iteration 33, loss = 0.20023820\n","Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 1.23714834\n","Iteration 2, loss = 0.79617487\n","Iteration 3, loss = 0.54574699\n","Iteration 4, loss = 0.52829059\n","Iteration 5, loss = 0.52482560\n","Iteration 6, loss = 0.43655159\n","Iteration 7, loss = 0.38987155\n","Iteration 8, loss = 0.35199913\n","Iteration 9, loss = 0.31045561\n","Iteration 10, loss = 0.29511117\n","Iteration 11, loss = 0.25207126\n","Iteration 12, loss = 0.25372468\n","Iteration 13, loss = 0.26825510\n","Iteration 14, loss = 0.25296694\n","Iteration 15, loss = 0.26034704\n","Iteration 16, loss = 0.25807734\n","Iteration 17, loss = 0.26035144\n","Iteration 18, loss = 0.25256024\n","Iteration 19, loss = 0.23857721\n","Iteration 20, loss = 0.24155343\n","Iteration 21, loss = 0.23657017\n","Iteration 22, loss = 0.24262141\n","Iteration 23, loss = 0.23728506\n","Iteration 24, loss = 0.23219202\n","Iteration 25, loss = 0.23384782\n","Iteration 26, loss = 0.25198293\n","Iteration 27, loss = 0.22340998\n","Iteration 28, loss = 0.21237508\n","Iteration 29, loss = 0.24885314\n","Iteration 30, loss = 0.22899693\n","Iteration 31, loss = 0.22274444\n","Iteration 32, loss = 0.20766654\n","Iteration 33, loss = 0.22333584\n","Iteration 34, loss = 0.22704026\n","Iteration 35, loss = 0.22011871\n","Iteration 36, loss = 0.25185681\n","Iteration 37, loss = 0.23462516\n","Iteration 38, loss = 0.23430654\n","Iteration 39, loss = 0.23730991\n","Iteration 40, loss = 0.22622056\n","Iteration 41, loss = 0.22679040\n","Iteration 42, loss = 0.22898558\n","Iteration 43, loss = 0.21681613\n","Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 1.24728870\n","Iteration 2, loss = 0.84975130\n","Iteration 3, loss = 0.60472627\n","Iteration 4, loss = 0.54217612\n","Iteration 5, loss = 0.50950252\n","Iteration 6, loss = 0.45658846\n","Iteration 7, loss = 0.43108235\n","Iteration 8, loss = 0.42196812\n","Iteration 9, loss = 0.38892725\n","Iteration 10, loss = 0.33425195\n","Iteration 11, loss = 0.30502402\n","Iteration 12, loss = 0.28535502\n","Iteration 13, loss = 0.27254556\n","Iteration 14, loss = 0.26544904\n","Iteration 15, loss = 0.25428359\n","Iteration 16, loss = 0.26345897\n","Iteration 17, loss = 0.27187742\n","Iteration 18, loss = 0.24753236\n","Iteration 19, loss = 0.24243593\n","Iteration 20, loss = 0.25288778\n","Iteration 21, loss = 0.24843986\n","Iteration 22, loss = 0.26214050\n","Iteration 23, loss = 0.27937518\n","Iteration 24, loss = 0.25252462\n","Iteration 25, loss = 0.27015188\n","Iteration 26, loss = 0.24227853\n","Iteration 27, loss = 0.23662266\n","Iteration 28, loss = 0.23496503\n","Iteration 29, loss = 0.23465835\n","Iteration 30, loss = 0.23699542\n","Iteration 31, loss = 0.22629501\n","Iteration 32, loss = 0.21896262\n","Iteration 33, loss = 0.24937933\n","Iteration 34, loss = 0.23963209\n","Iteration 35, loss = 0.24073237\n","Iteration 36, loss = 0.22625539\n","Iteration 37, loss = 0.22410002\n","Iteration 38, loss = 0.22634555\n","Iteration 39, loss = 0.23461337\n","Iteration 40, loss = 0.24028937\n","Iteration 41, loss = 0.23003219\n","Iteration 42, loss = 0.23077313\n","Iteration 43, loss = 0.23421774\n","Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 1.25311060\n","Iteration 2, loss = 0.77631484\n","Iteration 3, loss = 0.56810693\n","Iteration 4, loss = 0.54154128\n","Iteration 5, loss = 0.52778187\n","Iteration 6, loss = 0.51125137\n","Iteration 7, loss = 0.50638197\n","Iteration 8, loss = 0.48737149\n","Iteration 9, loss = 0.48471079\n","Iteration 10, loss = 0.43973793\n","Iteration 11, loss = 0.40080232\n","Iteration 12, loss = 0.37773732\n","Iteration 13, loss = 0.35715170\n","Iteration 14, loss = 0.33431616\n","Iteration 15, loss = 0.31203923\n","Iteration 16, loss = 0.32559930\n","Iteration 17, loss = 0.33060756\n","Iteration 18, loss = 0.28900383\n","Iteration 19, loss = 0.29590125\n","Iteration 20, loss = 0.29287652\n","Iteration 21, loss = 0.28154160\n","Iteration 22, loss = 0.27979062\n","Iteration 23, loss = 0.26992180\n","Iteration 24, loss = 0.30236564\n","Iteration 25, loss = 0.30075963\n","Iteration 26, loss = 0.27433572\n","Iteration 27, loss = 0.26646820\n","Iteration 28, loss = 0.25689442\n","Iteration 29, loss = 0.28593969\n","Iteration 30, loss = 0.28585415\n","Iteration 31, loss = 0.25300269\n","Iteration 32, loss = 0.27657444\n","Iteration 33, loss = 0.26018061\n","Iteration 34, loss = 0.26825267\n","Iteration 35, loss = 0.25578911\n","Iteration 36, loss = 0.27052379\n","Iteration 37, loss = 0.26625810\n","Iteration 38, loss = 0.25329415\n","Iteration 39, loss = 0.26577261\n","Iteration 40, loss = 0.25545740\n","Iteration 41, loss = 0.23123401\n","Iteration 42, loss = 0.27227662\n","Iteration 43, loss = 0.26714843\n","Iteration 44, loss = 0.30261000\n","Iteration 45, loss = 0.24662264\n","Iteration 46, loss = 0.25140473\n","Iteration 47, loss = 0.24906262\n","Iteration 48, loss = 0.24840271\n","Iteration 49, loss = 0.24629244\n","Iteration 50, loss = 0.25568907\n","Iteration 51, loss = 0.25001256\n","Iteration 52, loss = 0.23016708\n","Iteration 53, loss = 0.23575703\n","Iteration 54, loss = 0.24782232\n","Iteration 55, loss = 0.25718257\n","Iteration 56, loss = 0.23603993\n","Iteration 57, loss = 0.22526055\n","Iteration 58, loss = 0.26406865\n","Iteration 59, loss = 0.26592671\n","Iteration 60, loss = 0.25032592\n","Iteration 61, loss = 0.24825539\n","Iteration 62, loss = 0.23295088\n","Iteration 63, loss = 0.22275394\n","Iteration 64, loss = 0.23739108\n","Iteration 65, loss = 0.23277916\n","Iteration 66, loss = 0.23160161\n","Iteration 67, loss = 0.25736295\n","Iteration 68, loss = 0.25755831\n","Iteration 69, loss = 0.24990426\n","Iteration 70, loss = 0.23453900\n","Iteration 71, loss = 0.22076667\n","Iteration 72, loss = 0.23385874\n","Iteration 73, loss = 0.25586166\n","Iteration 74, loss = 0.25343936\n","Iteration 75, loss = 0.26149228\n","Iteration 76, loss = 0.24185918\n","Iteration 77, loss = 0.22756475\n","Iteration 78, loss = 0.22879871\n","Iteration 79, loss = 0.23440981\n","Iteration 80, loss = 0.24437410\n","Iteration 81, loss = 0.22735947\n","Iteration 82, loss = 0.23987693\n","Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 1.25948532\n","Iteration 2, loss = 0.69650640\n","Iteration 3, loss = 0.35369627\n","Iteration 4, loss = 0.29422853\n","Iteration 5, loss = 0.26824497\n","Iteration 6, loss = 0.28450399\n","Iteration 7, loss = 0.26539579\n","Iteration 8, loss = 0.24686232\n","Iteration 9, loss = 0.25883594\n","Iteration 10, loss = 0.24667892\n","Iteration 11, loss = 0.24515239\n","Iteration 12, loss = 0.25474876\n","Iteration 13, loss = 0.25116226\n","Iteration 14, loss = 0.24040887\n","Iteration 15, loss = 0.24499648\n","Iteration 16, loss = 0.26031072\n","Iteration 17, loss = 0.28368374\n","Iteration 18, loss = 0.24020822\n","Iteration 19, loss = 0.23164541\n","Iteration 20, loss = 0.23433590\n","Iteration 21, loss = 0.24034217\n","Iteration 22, loss = 0.25262562\n","Iteration 23, loss = 0.23375265\n","Iteration 24, loss = 0.23347009\n","Iteration 25, loss = 0.24372631\n","Iteration 26, loss = 0.22945821\n","Iteration 27, loss = 0.22745199\n","Iteration 28, loss = 0.23240739\n","Iteration 29, loss = 0.22601770\n","Iteration 30, loss = 0.24614352\n","Iteration 31, loss = 0.22733298\n","Iteration 32, loss = 0.23361788\n","Iteration 33, loss = 0.23970071\n","Iteration 34, loss = 0.22390191\n","Iteration 35, loss = 0.21761247\n","Iteration 36, loss = 0.22616896\n","Iteration 37, loss = 0.23158486\n","Iteration 38, loss = 0.22241985\n","Iteration 39, loss = 0.22359031\n","Iteration 40, loss = 0.21858775\n","Iteration 41, loss = 0.21150683\n","Iteration 42, loss = 0.21074846\n","Iteration 43, loss = 0.21172872\n","Iteration 44, loss = 0.20739595\n","Iteration 45, loss = 0.21114392\n","Iteration 46, loss = 0.23100085\n","Iteration 47, loss = 0.20270198\n","Iteration 48, loss = 0.21085626\n","Iteration 49, loss = 0.22483545\n","Iteration 50, loss = 0.20896334\n","Iteration 51, loss = 0.21635160\n","Iteration 52, loss = 0.22578069\n","Iteration 53, loss = 0.22728633\n","Iteration 54, loss = 0.21865632\n","Iteration 55, loss = 0.22801761\n","Iteration 56, loss = 0.21785878\n","Iteration 57, loss = 0.20372323\n","Iteration 58, loss = 0.23306828\n","Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 1.25815161\n","Iteration 2, loss = 0.88666917\n","Iteration 3, loss = 0.61280695\n","Iteration 4, loss = 0.45864409\n","Iteration 5, loss = 0.40034683\n","Iteration 6, loss = 0.36332323\n","Iteration 7, loss = 0.34084629\n","Iteration 8, loss = 0.30232248\n","Iteration 9, loss = 0.26765611\n","Iteration 10, loss = 0.26613315\n","Iteration 11, loss = 0.25658703\n","Iteration 12, loss = 0.25471350\n","Iteration 13, loss = 0.23836574\n","Iteration 14, loss = 0.24064435\n","Iteration 15, loss = 0.22793321\n","Iteration 16, loss = 0.24697798\n","Iteration 17, loss = 0.28139936\n","Iteration 18, loss = 0.23654063\n","Iteration 19, loss = 0.25958737\n","Iteration 20, loss = 0.24452151\n","Iteration 21, loss = 0.25325421\n","Iteration 22, loss = 0.25346145\n","Iteration 23, loss = 0.23724753\n","Iteration 24, loss = 0.23983863\n","Iteration 25, loss = 0.22430455\n","Iteration 26, loss = 0.22503063\n","Iteration 27, loss = 0.22612750\n","Iteration 28, loss = 0.23164492\n","Iteration 29, loss = 0.22207615\n","Iteration 30, loss = 0.23923441\n","Iteration 31, loss = 0.23748064\n","Iteration 32, loss = 0.24024756\n","Iteration 33, loss = 0.24971229\n","Iteration 34, loss = 0.22379694\n","Iteration 35, loss = 0.23015071\n","Iteration 36, loss = 0.24230517\n","Iteration 37, loss = 0.22929130\n","Iteration 38, loss = 0.23700783\n","Iteration 39, loss = 0.23881589\n","Iteration 40, loss = 0.22900747\n","Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 1.25292488\n","Iteration 2, loss = 0.81771235\n","Iteration 3, loss = 0.68011628\n","Iteration 4, loss = 0.56827808\n","Iteration 5, loss = 0.52152066\n","Iteration 6, loss = 0.52626603\n","Iteration 7, loss = 0.53077980\n","Iteration 8, loss = 0.49777377\n","Iteration 9, loss = 0.48438616\n","Iteration 10, loss = 0.44818463\n","Iteration 11, loss = 0.37485091\n","Iteration 12, loss = 0.35136818\n","Iteration 13, loss = 0.35156956\n","Iteration 14, loss = 0.34817926\n","Iteration 15, loss = 0.34090147\n","Iteration 16, loss = 0.34446145\n","Iteration 17, loss = 0.34483259\n","Iteration 18, loss = 0.32495314\n","Iteration 19, loss = 0.32470519\n","Iteration 20, loss = 0.32137930\n","Iteration 21, loss = 0.32186991\n","Iteration 22, loss = 0.32485301\n","Iteration 23, loss = 0.32147895\n","Iteration 24, loss = 0.33733866\n","Iteration 25, loss = 0.32081601\n","Iteration 26, loss = 0.28998833\n","Iteration 27, loss = 0.28138462\n","Iteration 28, loss = 0.28838607\n","Iteration 29, loss = 0.28081426\n","Iteration 30, loss = 0.28567505\n","Iteration 31, loss = 0.27759815\n","Iteration 32, loss = 0.27205551\n","Iteration 33, loss = 0.28549921\n","Iteration 34, loss = 0.28638049\n","Iteration 35, loss = 0.26625933\n","Iteration 36, loss = 0.26391527\n","Iteration 37, loss = 0.26450574\n","Iteration 38, loss = 0.28076631\n","Iteration 39, loss = 0.26311845\n","Iteration 40, loss = 0.25671527\n","Iteration 41, loss = 0.25660173\n","Iteration 42, loss = 0.26725684\n","Iteration 43, loss = 0.25769420\n","Iteration 44, loss = 0.28272290\n","Iteration 45, loss = 0.26606145\n","Iteration 46, loss = 0.26379953\n","Iteration 47, loss = 0.25826463\n","Iteration 48, loss = 0.23127117\n","Iteration 49, loss = 0.22613507\n","Iteration 50, loss = 0.21527169\n","Iteration 51, loss = 0.22902186\n","Iteration 52, loss = 0.23322640\n","Iteration 53, loss = 0.24366087\n","Iteration 54, loss = 0.21187588\n","Iteration 55, loss = 0.22575723\n","Iteration 56, loss = 0.21814388\n","Iteration 57, loss = 0.21473831\n","Iteration 58, loss = 0.24121356\n","Iteration 59, loss = 0.24758554\n","Iteration 60, loss = 0.22961207\n","Iteration 61, loss = 0.21082165\n","Iteration 62, loss = 0.20872307\n","Iteration 63, loss = 0.20581578\n","Iteration 64, loss = 0.21891949\n","Iteration 65, loss = 0.23355642\n","Iteration 66, loss = 0.23946870\n","Iteration 67, loss = 0.21080019\n","Iteration 68, loss = 0.22385361\n","Iteration 69, loss = 0.22443340\n","Iteration 70, loss = 0.22523497\n","Iteration 71, loss = 0.21037770\n","Iteration 72, loss = 0.21657403\n","Iteration 73, loss = 0.22912336\n","Iteration 74, loss = 0.23546246\n","Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 1.25246991\n","Iteration 2, loss = 0.49159118\n","Iteration 3, loss = 0.28624623\n","Iteration 4, loss = 0.25534874\n","Iteration 5, loss = 0.23433610\n","Iteration 6, loss = 0.22843163\n","Iteration 7, loss = 0.20935285\n","Iteration 8, loss = 0.22572562\n","Iteration 9, loss = 0.22399214\n","Iteration 10, loss = 0.20922103\n","Iteration 11, loss = 0.21440059\n","Iteration 12, loss = 0.22035854\n","Iteration 13, loss = 0.20882820\n","Iteration 14, loss = 0.20654584\n","Iteration 15, loss = 0.19630656\n","Iteration 16, loss = 0.19739346\n","Iteration 17, loss = 0.18943601\n","Iteration 18, loss = 0.19725156\n","Iteration 19, loss = 0.19895132\n","Iteration 20, loss = 0.20350234\n","Iteration 21, loss = 0.18011591\n","Iteration 22, loss = 0.18576427\n","Iteration 23, loss = 0.20484527\n","Iteration 24, loss = 0.21344281\n","Iteration 25, loss = 0.20168545\n","Iteration 26, loss = 0.20379366\n","Iteration 27, loss = 0.20465181\n","Iteration 28, loss = 0.20224592\n","Iteration 29, loss = 0.19143019\n","Iteration 30, loss = 0.19087868\n","Iteration 31, loss = 0.17470511\n","Iteration 32, loss = 0.18341966\n","Iteration 33, loss = 0.18184186\n","Iteration 34, loss = 0.18119094\n","Iteration 35, loss = 0.18275527\n","Iteration 36, loss = 0.18718173\n","Iteration 37, loss = 0.20499611\n","Iteration 38, loss = 0.22920809\n","Iteration 39, loss = 0.19522622\n","Iteration 40, loss = 0.22227343\n","Iteration 41, loss = 0.22207284\n","Iteration 42, loss = 0.19409137\n","Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 1.24758261\n","Iteration 2, loss = 0.76287373\n","Iteration 3, loss = 0.49983198\n","Iteration 4, loss = 0.33069578\n","Iteration 5, loss = 0.21789565\n","Iteration 6, loss = 0.22879455\n","Iteration 7, loss = 0.21484145\n","Iteration 8, loss = 0.23542921\n","Iteration 9, loss = 0.19196857\n","Iteration 10, loss = 0.19244177\n","Iteration 11, loss = 0.19641427\n","Iteration 12, loss = 0.20115430\n","Iteration 13, loss = 0.19006356\n","Iteration 14, loss = 0.17032888\n","Iteration 15, loss = 0.17472672\n","Iteration 16, loss = 0.18545545\n","Iteration 17, loss = 0.18251339\n","Iteration 18, loss = 0.16518086\n","Iteration 19, loss = 0.17796130\n","Iteration 20, loss = 0.17178529\n","Iteration 21, loss = 0.15597157\n","Iteration 22, loss = 0.16110504\n","Iteration 23, loss = 0.16403268\n","Iteration 24, loss = 0.16206277\n","Iteration 25, loss = 0.18325173\n","Iteration 26, loss = 0.15324841\n","Iteration 27, loss = 0.16274763\n","Iteration 28, loss = 0.16801800\n","Iteration 29, loss = 0.16615837\n","Iteration 30, loss = 0.15795873\n","Iteration 31, loss = 0.17870494\n","Iteration 32, loss = 0.20738320\n","Iteration 33, loss = 0.17883786\n","Iteration 34, loss = 0.17846901\n","Iteration 35, loss = 0.18921087\n","Iteration 36, loss = 0.16592396\n","Iteration 37, loss = 0.15514534\n","Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n","Experimento 16\n","Acurácia Média: 88.93%\n","Precisão Média: 91.96%\n","Revocação Média: 90.52%\n","F1-Score Médio: 90.13%\n"]}],"source":["from sklearn.neural_network import MLPClassifier\n","from sklearn.model_selection import cross_validate\n","\n","\n","x_train = df_scaled.iloc[:, 0:16]\n","y_train = df_scaled.iloc[:, 16]\n","classifier = MLPClassifier(activation='logistic', solver='adam', alpha=1e-5, hidden_layer_sizes=(12, 3), random_state=1, verbose=True, learning_rate_init=0.3, tol=1e-3, max_iter=500)\n","scoring = {'acc' : 'accuracy',\n","           'prec' : 'precision_macro',\n","           'recall' : 'recall_macro',\n","           'f1' : 'f1_macro'}\n","\n","\n","y_pred = cross_validate(classifier, x_train, y_train, cv=10, scoring=scoring, return_train_score=True)\n","print('Experimento 16')\n","print('Acurácia Média: ' + '%.2f' % (np.mean(y_pred['test_acc'])*100) + '%')\n","print('Precisão Média: ' + '%.2f' % (np.mean(y_pred['test_prec'])*100) + '%')\n","print('Revocação Média: ' + '%.2f' % (np.mean(y_pred['test_recall'])*100) + '%')\n","print('F1-Score Médio: ' + '%.2f' % (np.mean(y_pred['test_f1'])*100) + '%')\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNUI5OPrQl7NiHnDUB4L5jS","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
